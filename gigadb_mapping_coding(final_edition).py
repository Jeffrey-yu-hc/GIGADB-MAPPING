# -*- coding: utf-8 -*-
"""GIGADB MAPPING CODING(FINAL EDITION).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kkf36UdU4BIdP_bmBme4swETCNpZy_uo

Usage of this code:

In this code, you need to install the first package called 'requests', which is for calling the API in python. Here are the 2 ways that you can install it.
First, if you want you use it time and again, please install it in your computer terminal, If you are using macbook/mac/sort of apple product, please open your Terminal, and copy this line: pip install requests. If you are using windows, please open your Command Prompt, and copy the same line.

Since it is python, all python supportive platform could be used to run this program theoretically, and I recommend you might use Google Colab, online cloud platform, no need to install, here is the link: https://colab.research.google.com/

As you run it, there are json format file creating as following, if you use Google Colab, at the left hand side, you could them under the 'Files'.
"""

import requests # This is for API call
import xml.etree.ElementTree as ET # This is for XML format
import re # This is to examine the format of 'ftp site' in datacite 4.5
import copy # This is for copy the data template
import datetime # This is for getting the current date for the 'dates' in datacite 4.5
import json # This is for the json file creation
import os


def get_doi_list(url):
    response_doi_link = requests.get(url)
    doi_list = []
    if response_doi_link.status_code == 200:
        doi_list = re.findall(r'<doi prefix="10\.5524">(\d+)</doi>', response_doi_link.text)
    else:
        print("Failed to retrieve the XML file.")
    return doi_list

# This part is to really make the 6 digits doi number of gigadb to a clickable links, however, in our code, it is no longer be used but kept.
#def fetch_dataset_responses(doi_list):
    #responses = []
    #for doi_number in doi_list:
        #response = f'https://gigadb.org/api/dataset?doi={doi_number}'
        #if response:
            #responses.append(response)
        #else:
            #print(f'Failed to retrieve dataset for DOI:{doi_number}')
            #responses.append(None)  # If the request fails, append None or another marker to the list
    #print(responses)
    #return responses  # Ensure you return the responses list

# Now use the function to get the DOI list and fetch dataset responses
#doi_list = get_doi_list('https://gigadb.org/api/list')
#dataset_responses = fetch_dataset_responses(doi_list)


##This part are the extraction functions, aims in getting the wanted information from the original gigadb file.
# Part for title//
def clean_xml_content_title(xml_content):
    # Find the starting and ending positions of the <title> tag
    start_index_title = xml_content.find('<title>')
    end_index_title = xml_content.find('</title>') + len('</title>')

    if start_index_title != -1 and end_index_title != -1:
        # Extract the <title> tag and its content
        return xml_content[start_index_title:end_index_title]
    else:
        # Raise an error if the <title> tag is not found properly
        raise ValueError("The <title> tag was not found properly in the XML content.")

def extract_title(cleaned_content):
    try:
        # Parse the cleaned XML content
        root = ET.fromstring(cleaned_content)
        # Extract and return the text of the <title> element
        return root.text.strip() if root.text else "No title found"
    except ET.ParseError as e:
        # Raise an error if parsing the XML content fails
        raise ValueError("Failed to parse the XML content. Please check the XML structure.") from e
# Part for title//


# Part for keywords//
def clean_xml_content_ds_attributes(xml_content):
    # Ensure the XML content starts with the <ds_attributes> root element
    start_index_ds_attributes = xml_content.find('<ds_attributes>')
    end_index_ds_attributes = xml_content.find('</ds_attributes>') + len('</ds_attributes>')

    if start_index_ds_attributes != -1 and end_index_ds_attributes != -1:
        # Extract the <ds_attributes> element and its content, wrap it with a root element
        cleaned_content = xml_content[start_index_ds_attributes:end_index_ds_attributes]
        return f"<root>{cleaned_content}</root>"
    else:
        # Print a warning if the <ds_attributes> tag is not found and return an empty root element
        print("Warning: The <ds_attributes> tag was not found properly in the XML content.")
        return "<root></root>"

def extract_keys(xml_content):
    # Extract text from <key> elements within the XML content
    try:
        # Parse the XML content
        root = ET.fromstring(xml_content)
    except ET.ParseError:
        # Print a warning if parsing fails and return an empty root element
        print("Warning: Failed to parse the XML content. Please check the XML structure.")
        root = ET.Element("root")

    keys = []
    # Find all <attribute> elements and extract the text from their <value> child elements
    for attribute in root.findall('.//attribute'):
        key_elem = attribute.find('.//value')
        if key_elem is not None and key_elem.text:
            keys.append(key_elem.text.strip())

    # Return the list of extracted keys
    return keys
# Part for keywords//


# Part for ftp site//
def clean_xml_content_ftp_site(xml_content):
    # Clean the XML content to ensure it only contains the <ftp_site> element
    start_index_ftp_site = xml_content.find('<ftp_site>')
    end_index_ftp_site = xml_content.find('</ftp_site>') + len('</ftp_site>')

    if start_index_ftp_site != -1 and end_index_ftp_site != -1:
        # Extract the <ftp_site> element and its content
        return xml_content[start_index_ftp_site:end_index_ftp_site]
    else:
        # Raise an error if the <ftp_site> tag is not found properly
        raise ValueError("The <ftp_site> tag was not found properly in the XML content.")

def extract_ftp_site(xml_content):
    try:
        # Parse the XML content, wrapping it with a root element
        root = ET.fromstring(f'<root>{xml_content}</root>')
    except ET.ParseError as e:
        # Raise an error if parsing the XML content fails
        raise ValueError("Failed to parse the XML content. Please check the XML structure.") from e

    # Find the <ftp_site> element and return its text if it exists
    ftp_site = root.find('.//ftp_site')
    if ftp_site is not None and ftp_site.text:
        return ftp_site.text.strip()
    else:
        # Return a default message if no <ftp_site> element is found
        return "No ftp_site found"
# Part for ftp site//



# Part for file//
def clean_xml_content_files(xml_content):
    # Locate the <files> section within the XML content
    start_index_files = xml_content.find('<files>')  # Find the starting index of <files>
    end_index_files = xml_content.find('</files>') + len('</files>')  # Find the ending index of </files>

    # Check if both start and end indices of <files> are found
    if start_index_files != -1 and end_index_files != -1:
        # Extract and return the portion of XML that includes <files> and its content
        return xml_content[start_index_files:end_index_files]
    else:
        # Raise an error if the <files> element cannot be properly located
        raise ValueError("The <files> tag was not found properly in the XML content.")

def count_files(root):
    # Count the number of <file> elements in the XML
    files = root.findall('.//file')  # Find all <file> elements in the XML tree
    return len(files)  # Return the number of <file> elements found

def extract_files(xml_content):
    cleaned_xml_content_files = clean_xml_content_files(xml_content)  # Clean XML content to include only <files>
    try:
        root = ET.fromstring(cleaned_xml_content_files)  # Parse the cleaned XML content into an XML tree
        file_count = count_files(root)  # Count the number of <file> elements in the XML tree
        return file_count  # Return the count of <file> elements
    except ET.ParseError as e:
        # Raise an error if the XML cannot be parsed correctly
        raise ValueError("Failed to parse the XML content. Please check the XML structure.") from e
# Part for file//



# Part for description//
def clean_xml_content_description(xml_content):
    # Remove the XML declaration to simplify the XML content handling.
    if xml_content.startswith('<?xml'):
        xml_content = xml_content.split('?>', 1)[1]

    # Locate the starting and ending indices of the <description> element.
    start_index = xml_content.find('<description>')
    end_index = xml_content.find('</description>') + len('</description>')

    # If both start and end tags are found, extract the content between them.
    if start_index != -1 and end_index != -1:
        cleaned_content = xml_content[start_index:end_index]
    else:
        # If the tags are not correctly found, raise an error.
        raise ValueError("The <description> tag was not found properly in the XML content.")

    # Wrap the cleaned content with a root element to maintain valid XML structure for parsing.
    return f"<root>{cleaned_content}</root>"

def extract_description(xml_content):
    try:
        # Parse the XML content to create an XML tree structure.
        root = ET.fromstring(xml_content)
        # Search for the <description> element within the XML tree.
        description_elem = root.find('.//description')
        if description_elem is not None:
            # Return the text of the description element, stripping any leading/trailing whitespace.
            return description_elem.text.strip() if description_elem.text else "Description is empty"
        else:
            # If no description element is found, return a default message.
            return "No description found"
    except ET.ParseError as e:
        # If there is an error in parsing the XML, raise a ValueError with a descriptive message.
        raise ValueError("Failed to parse the XML content. Please check the XML structure.") from e
# Part for description//



# Part for author//
def clean_xml_content_authors(xml_content):
    # Remove XML declaration to simplify processing the content
    if xml_content.startswith('<?xml'):
        xml_content = xml_content.split('?>', 1)[1]

    # Define the start and end tags for the <authors> element
    start_index_authors = xml_content.find('<authors>')
    end_index_authors = xml_content.find('</authors>') + len('</authors>')

    # Check if the start and end tags for <authors> are found within the content
    if start_index_authors != -1 and end_index_authors != -1:
        # If found, return the substring that contains the <authors> element and its content
        return xml_content[start_index_authors:end_index_authors]
    else:
        # If the tags are not found, raise an error indicating the issue
        raise ValueError("The <authors> tag was not found properly in the XML content.")

def extract_authors(xml_content):
    authors_data = []  # Initialize an empty list to store author information

    try:
        cleaned_xml_content_authors = clean_xml_content_authors(xml_content)  # Clean the XML content to include only <authors>
        root = ET.fromstring(cleaned_xml_content_authors)  # Parse the cleaned XML content into an XML tree

        # Find all <author> elements within the <authors> element
        authors = root.findall('.//author')
        for author in authors:
            # Extract the text contents of 'firstname', 'middlename', 'surname', 'orcid', and 'organization'
            # Strip any leading/trailing whitespace from each field
            author_info = {
                "firstname": author.findtext('firstname', default='').strip(),
                "middlename": author.findtext('middlename', default='').strip(),
                "surname": author.findtext('surname', default='').strip(),
                "orcid": author.findtext('orcid', default='').strip(),
                # Include 'organization' field only if author tag is "Consortium"
                "organization": author.findtext('organization', default='').strip() if author.tag == "Consortium" else ""
            }
            # Append the extracted information to the authors_data list
            authors_data.append(author_info)
    except ET.ParseError as e:
        # Raise an error if there is a problem parsing the XML structure
        raise ValueError("Failed to parse the XML content. Please check the XML structure.") from e
    except ValueError as ve:
        # Propagate any other exceptions that occur
        raise ve

    # Return the list of authors' information
    return authors_data
# Part for author//



# Part for data type//
def clean_xml_content_data_types(xml_content):
    # Remove XML declaration to simplify processing the content
    if xml_content.startswith('<?xml'):
        xml_content = xml_content.split('?>', 1)[1]

    # Define the start and end tags for the <data_types> element
    start_index_data_types = xml_content.find('<data_types>')
    end_index_data_types = xml_content.find('</data_types>') + len('</data_types>')

    # Check if both start and end tags for <data_types> are found
    if start_index_data_types != -1 and end_index_data_types != -1:
        # If found, return the substring that contains the <data_types> element and its contents
        return xml_content[start_index_data_types:end_index_data_types]
    else:
        # If the tags are not found, raise an error indicating the issue
        raise ValueError("The <data_types> tag was not found properly in the XML content.")

def extract_data_types(xml_content):
    data_types_data = []  # Initialize an empty list to store data type information

    try:
        cleaned_xml_content = clean_xml_content_data_types(xml_content)  # Clean the XML content to include only <data_types>
        root = ET.fromstring(cleaned_xml_content)  # Parse the cleaned XML content into an XML tree

        # Find all <dataset_type> elements within the <data_types> element
        dataset_types = root.findall('.//dataset_type')
        for dataset_type in dataset_types:
            # Extract the text contents of the 'type_name' and 'type_id' elements, stripping any leading/trailing whitespace
            data_type_info = {
                "type_name": dataset_type.findtext('type_name', default='').strip(),
                "type_id": dataset_type.findtext('type_id', default='').strip()
            }
            # Append the extracted information to the data_types_data list
            data_types_data.append(data_type_info)
    except ET.ParseError as e:
        # Raise an error if there is a problem parsing the XML structure
        raise ValueError("Failed to parse the XML content. Please check the XML structure.") from e
    except ValueError as ve:
        # Propagate any other exceptions that occur
        raise ve

    # Return the list of data type information
    return data_types_data
# Part for data type//


# Part for daset size//
def clean_xml_content_dataset_size(xml_content):
    # Remove the XML declaration to ensure only the <dataset_size> element is considered
    if xml_content.startswith('<?xml'):
        # Split off the XML declaration and keep the rest of the content
        xml_content = xml_content.split('?>', 1)[1]

    # Define the start and end tags for the <dataset_size> element
    start_tag_dataset_size = '<dataset_size'
    end_tag_dataset_size = '</dataset_size>'
    # Locate the positions of the start and end tags within the XML content
    start_index_dataset_size = xml_content.find(start_tag_dataset_size)
    end_index_dataset_size = xml_content.find(end_tag_dataset_size) + len(end_tag_dataset_size)

    # Check if both start and end tags were found
    if start_index_dataset_size != -1 and end_index_dataset_size != -1:
        # Extract and return the content within these tags, wrapped in a root element for valid XML structure
        return f"<root>{xml_content[start_index_dataset_size:end_index_dataset_size]}</root>"
    else:
        # Raise an error if the <dataset_size> tag was not found correctly
        raise ValueError("The <dataset_size> tag was not found properly in the XML content.")

def extract_dataset_size(xml_content):
    # Clean the XML content to isolate the <dataset_size> element
    cleaned_xml_content = clean_xml_content_dataset_size(xml_content)
    # Try to parse the cleaned XML content into an XML tree
    try:
        root = ET.fromstring(cleaned_xml_content)
    except ET.ParseError as e:
        # If parsing fails, raise an error with advice to check the XML structure
        raise ValueError("Failed to parse the XML content. Please check the XML structure.") from e

    # Attempt to find the <dataset_size> node within the XML tree
    dataset_size_node = root.find('.//dataset_size')
    if dataset_size_node is not None:
        # If the node is found, extract and return its text content and attribute (units)
        return {
            "size": dataset_size_node.text.strip(),  # Get the size value, stripping any whitespace
            "units": dataset_size_node.attrib.get('units', '').strip()  # Get the units attribute, stripping whitespace
        }
    # Return None if the <dataset_size> node is not found
    return None
# Part for daset size//



## Part for Publication//
def clean_xml_content_publication(xml_content):

    # Check if the content starts with an XML declaration and remove it
    if xml_content.startswith('<?xml'):
        xml_content = xml_content.split('?>', 1)[1]

    # Define the start and end tags for the <publication> element
    start_tag_publication = '<publication'
    end_tag_publication = '</publication>'
    # Locate the indices for the start and end tags within the content
    start_index_publication = xml_content.find(start_tag_publication)
    end_index_publication = xml_content.find(end_tag_publication) + len(end_tag_publication)

    # Check if both start and end indices are found
    if start_index_publication != -1 and end_index_publication != -1:
        # Extract the publication content and wrap it in a <root> element for proper XML structure
        return f"<root>{xml_content[start_index_publication:end_index_publication]}</root>"
    else:
        # Raise an error if the publication tags are not found
        raise ValueError("The <publication> tag was not found properly in the XML content.")

def extract_publication(xml_content):

    # Clean the XML content to ensure it is formatted correctly
    cleaned_xml_content = clean_xml_content_publication(xml_content)

    # Attempt to parse the cleaned XML content
    try:
        root = ET.fromstring(cleaned_xml_content)
    except ET.ParseError as e:
        # Raise an error if parsing fails
        raise ValueError("Failed to parse the XML content. Please check the XML structure.") from e

    # Search for the publication node within the XML structure
    publication_node = root.find('.//publication')
    if publication_node is not None:
        # Extract publication details and return them as a dictionary
        return {
            "date": publication_node.attrib.get('date', ''),
            "publisher_name": publication_node.find('publisher').attrib.get('name', '') if publication_node.find('publisher') is not None else '',
            "modification_date": publication_node.findtext('modification_date', default=''),
            "fair_use_date": publication_node.find('fair_use').attrib.get('date', '') if publication_node.find('fair_use') is not None else '',
        }
    # Return None if no publication node is found
    return None
## Part for Publication//



# Part for relatedIdentifier//
def clean_and_extract_xml_content_links(xml_content, tag_name):
    # Remove the XML declaration and clean the XML content, ensuring it contains only the specified node and its content
    if xml_content.startswith('<?xml'):
        xml_content = xml_content.split('?>', 1)[1]

    # Define the start and end tags for the specified node
    start_tag_links = f'<{tag_name}>'
    end_tag_links = f'</{tag_name}>'
    start_index_links = xml_content.find(start_tag_links)
    end_index_links = xml_content.find(end_tag_links) + len(end_tag_links)

    # Check if the start and end tags are found in the XML content
    if start_index_links != -1 and end_index_links != -1:
        # Extract the content between the start and end tags and wrap it in a root element
        cleaned_content_links = xml_content[start_index_links:end_index_links]
        cleaned_content_links = f"<root>{cleaned_content_links}</root>"
        return cleaned_content_links
    else:
        # Raise an error if the specified tags are not found
        raise ValueError(f"The <{tag_name}> tag was not found properly in the XML content.")

def extract_manuscript_links(source_root):
    # Initialize a list to hold manuscript link data
    manuscript_links_data = []
    # Find all 'manuscript_link' elements in the XML tree
    manuscript_links = source_root.findall('.//manuscript_link')
    for manuscript_link in manuscript_links:
        # Create a dictionary with manuscript link information
        link_info = {
            "manuscript_DOI": manuscript_link.findtext('manuscript_DOI', default=''),
            "manuscript_pmid": manuscript_link.findtext('manuscript_pmid', default='')
        }
        # Append the dictionary to the list
        manuscript_links_data.append(link_info)
    # Return the list of manuscript link data
    return manuscript_links_data

def extract_external_links(source_root):
    # Initialize a list to hold external link data
    external_links_data = []
    # Find all 'external_link' elements in the XML tree
    external_links = source_root.findall('.//external_link')
    for external_link in external_links:
        # Create a dictionary with external link information
        link_info = {
            "type": external_link.attrib.get('type', ''),
            "text": external_link.text
        }
        # Append the dictionary to the list
        external_links_data.append(link_info)
    # Return the list of external link data
    return external_links_data

def extract_grants(source_root):
    # Initialize a list to hold grant data
    grants_data = []
    # Find all 'grant' elements in the XML tree
    grants = source_root.findall('.//grant')
    for grant in grants:
        # Create a dictionary with grant information
        grant_info = {
            "funder_name": grant.findtext('funder_name', default=''),
            "fundref_url": grant.findtext('fundref_url', default=''),
            "award": grant.findtext('award', default=''),
            "comment": grant.findtext('comment', default='')
        }
        # Append the dictionary to the list
        grants_data.append(grant_info)
    # Return the list of grant data
    return grants_data

def extract_alternative_identifiers(source_root):
    # Initialize a list to hold alternative identifier data
    alternative_identifiers_data = []
    # Find all 'alternative_identifier' elements in the XML tree
    alternative_identifiers = source_root.findall('.//alternative_identifier')
    for alternative_identifier in alternative_identifiers:
        # Create a dictionary with alternative identifier information
        identifier_info = {
            "is_primary": alternative_identifier.attrib.get('is_primary', '0'),
            "prefix": alternative_identifier.attrib.get('prefix', ''),
            "link": alternative_identifier.text
        }
        # Append the dictionary to the list
        alternative_identifiers_data.append(identifier_info)
    # Return the list of alternative identifier data
    return alternative_identifiers_data

def extract_project_links(source_root):
    # Initialize a list to hold project link data
    project_links_data = []
    # Find all 'project_link' elements in the XML tree
    project_links = source_root.findall('.//project_link')
    for project_link in project_links:
        # Create a dictionary with project link information
        project_link_info = {
            "project_name": project_link.findtext('project_name', default=''),
            "project_link": project_link.findtext('project_url', default='')
        }
        # Append the dictionary to the list
        project_links_data.append(project_link_info)
    # Return the list of project link data
    return project_links_data
# Part for relatedIdentifier//



# Function for Subject//
def clean_and_extract_xml_content_sample(xml_content, tag_name):
    # Remove XML declaration and extract the content of the specified node
    if xml_content.startswith('<?xml'):
        xml_content = xml_content.split('?>', 1)[1]

    # Define the start and end tags for the specified node
    start_tag_sample = f'<{tag_name}>'
    end_tag_sample = f'</{tag_name}>'
    start_index_sample = xml_content.find(start_tag_sample)
    end_index_sample = xml_content.find(end_tag_sample) + len(end_tag_sample)

    # Check if the start and end tags are found in the XML content
    if start_index_sample != -1 and end_index_sample != -1:
        # Extract the content between the start and end tags and wrap it in a root element
        cleaned_content_sample = xml_content[start_index_sample:end_index_sample]
        cleaned_content_sample = f"<root>{cleaned_content_sample}</root>"
        return cleaned_content_sample
    else:
        # Raise an error if the specified tags are not found
        raise ValueError(f"The <{tag_name}> tag was not found properly in the XML content.")

def extract_sample_name(xml_content):
    # Attempt to parse the XML content into an XML tree structure
    try:
        root = ET.fromstring(xml_content)
    except ET.ParseError as e:
        # Raise an error if the XML content cannot be parsed
        raise ValueError("Failed to parse the XML content. Please check the XML structure.") from e

    sample_data = []
    # Find all 'sample' elements in the XML tree
    samples = root.findall('.//sample')
    for sample in samples:
        # Extract the 'species' sub-element from each 'sample'
        species = sample.find('.//species')
        # Create a dictionary with sample information
        sample_info = {
            "name": sample.findtext('name', default=''),
            "species": {
                "tax_id": species.findtext('tax_id', default=''),
                "common_name": species.findtext('common_name', default=''),
                "genbank_name": species.findtext('genbank_name', default=''),
                "scientific_name": species.findtext('scientific_name', default=''),
            }
        }
        # Append the sample information to the list
        sample_data.append(sample_info)
    # Return the list of sample data
    return sample_data
# Function for Subject//


def process_links(xml_content):
    """
    Cleans and extracts the 'links' section from the XML content.
    It then parses the cleaned XML content to extract various types of links such as
    manuscript, external, grants, alternative identifiers, and project links.
    """

    # Clean and extract the 'links' section from the XML content
    cleaned_content = clean_and_extract_xml_content_links(xml_content, 'links')

    # Attempt to parse the cleaned XML content
    try:
        root = ET.fromstring(cleaned_content)
    except ET.ParseError as e:
        # If there is a parsing error, raise an exception with a message
        raise ValueError("Failed to parse the XML content. Please check the XML structure.") from e

    global manuscript_links_extracted
    global external_links_extracted
    global grants_extracted
    global alternative_identifiers_extracted
    global project_links_extracted

    # Extract various links and other related data from the XML tree
    manuscript_links_extracted = extract_manuscript_links(root)
    external_links_extracted = extract_external_links(root)
    grants_extracted = extract_grants(root)
    alternative_identifiers_extracted = extract_alternative_identifiers(root)
    project_links_extracted = extract_project_links(root)

    # Return a dictionary containing all extracted information
    return {
        "Manuscript Links": manuscript_links_extracted,
        "External Links": external_links_extracted,
        "Grants": grants_extracted,
        "Alternative Identifiers": alternative_identifiers_extracted,
        "Project Links": project_links_extracted
    }


def process_samples(xml_content):
    # Clean and extract the 'samples' section from the XML content
    # The reason for setting them as an individual function is their multiple information
    cleaned_content = clean_and_extract_xml_content_sample(xml_content, 'samples')

    # Extract and return sample names from the cleaned content
    return extract_sample_name(cleaned_content)

def match_ftp_site_patterns(ftp_site):
    # Define a list of regular expression patterns to match FTP site strings
    patterns = [r'pub/(\d+\.\d+)/\d+_\d+/(\d+)/', r'pub/(\d+\.\d+)/\d+_\d+/(\d+)']

    # Iterate over each pattern to find a match in the FTP site string
    for pattern in patterns:
        match = re.search(pattern, ftp_site)
        if match:
            # If a match is found, return the captured groups as a tuple
            return (match.group(1), match.group(2))

    # Return (None, None) if no match is found
    return (None, None)

#This part is for the set up of foundation of DataCite 4.5
root = ET.Element("dataset")

data_template = {
    "schema":{
    "data":{
        "id": "",

        "type": "",

        "attributes":
         {
            "doi": "",
            "prefix": "",
            "suffix": "",
         },

        "creators": {
                "familyName": "",
                "givenName": "",
                "nameType": "",
                "nameIdentifiers":[
                    {
                        "nameIdentifier": "",  #ORCID, it is a nameIdentifier
                        "nameIdentifierScheme": "",
                        "schemeURI": ""
                    }
                ]
            },

    "titles": {
        "lang":"",
        "title": ""
    },

    "publishers": {
        "lang":"",
        "name": "",
        "schemeURI": "",
        "publisherIdentifier": "",
        "publisherIdentifierScheme": "",
    },

    "publicationYear":"",

    "subjects": {
        "subject": "",
        "subject valueURI": "",
        "schemeURI": "",
        "subjectScheme": "",
        "classificationCode": "",
    },

    "dates": {
        "date": "",
        "dateType": ""
    },

    "language": "",

    "relatedIdentifiers": {
        "relatedIdentifier": "",
        "resourceTypeGeneral": "",
        "relationType": "",
        "relatedIdentifierType": "",
        "relatedItem": "",
        "relatedItemType": ""
    },

    "sizes": {
        "size": ""
    },

    "rightsList":{

        "lang": "",
        "rights": "",
        "rightsUri": "",
        "rightsIdentifier": ""
    },

    "descriptions": {
        "description": "",
        "descriptionType": ""
    },
    #"fundingReferences": {
       # "funderName": "",
        #"funderIdentifier": "",
        #"awardNumber": "",
        #"awardTitle": "",
        #"funderIdentifierType": ""}
    }
}
}



def populate_complete_data_structure(
    title_extracted, keywords_extracted, ftp_site_extracted, file_count, description_extracted, authors_extracted, types_extracted,
    dataset_size_extracted, publication_extracted, manuscript_links_extracted, external_links_extracted, alternative_identifiers_extracted,
    project_links_extracted,  grants_extracted, samples_extracted, data_template
):
    # Use deep copy to get the data template for following mapping
    data = copy.deepcopy(data_template)

    # FTP SITE part
    if ftp_site_extracted:
        identifier_info = ftp_site_extracted
        data['schema']['data']['attributes'] = {}  # A dict


        data['schema']['data']['id'] = prefix + "/" + suffix  # Use prefix and suffix as the id
        data['schema']['data']['type'] = "dois" # Hard coded, won't change whatever

        # A dict to store attributes data
        attributes_data = {
            "doi": prefix + "/" + suffix,
            "prefix": prefix,
            "suffix": suffix
        }
        # Add base fields directly to the attributes dict
        data['schema']['data']['attributes'].update(attributes_data)


    # Authors part
    if authors_extracted:
        # A dict
        data['schema']['data']['creators'] = []
        for author in authors_extracted: # A loop

            # condition for 'organizational' input
            name_type = "organizational" if "Consortium" in author.get("surname", "") else "personal"
            # Get the middle and orcid and put it in strip
            middlename = author.get('middlename', '').strip()
            orcid = author.get("orcid", "")
            if middlename:
               # if there is a middle name, get it
               givenName = f"{author.get('firstname')} {middlename}"
            else:
               # if not, just get the firstname
               givenName = author.get('firstname')
            # A dict
            author_data = {
            "familyName": author.get("surname", ""),
            "givenName": givenName,
            "nameType": name_type
        }
            if orcid: # if have orcid, then contain nameIdentifiers, if not, remove it
              author_data["nameIdentifiers"] = [
                {
                    "nameIdentifier": orcid,
                    "nameIdentifierScheme": "ORCID",
                    "schemeURI": "http://orcid.org/"
                }
            ]
            # Add the dicts
            data['schema']['data']['creators'].append(author_data)


    #Title part
    if title_extracted:
        data['schema']['data']['titles'] = []  # 初始化标题列表
        title_dict = {
            "lang": "en",
            "title": title_extracted  # 安全访问'title'键，如果不存在则返回空字符串
        }
        data['schema']['data']['titles'].append(title_dict)

    #Publisher part
    if publication_extracted:
        # A dict
        data['schema']['data']['publishers'] = []
        # Most parts of publisher data are hard coded, won't be changed anytime
        publisher_data = {
            "lang": "en",
            "name": publication_extracted.get('publisher_name', ""),
            "schemeURI": "https://www.re3data.org/",
            "publisherIdentifier": "http://doi.org/10.17616/R3TG83",
            "publisherIdentifierScheme": "re3data",
        }
        data['schema']['data']['publishers'].append(publisher_data)

        # Get the publication year
        publicationyear_data={
        "publicationYear": publication_extracted.get('date', "").split('-')[0]
        }
        data['schema']['data']['publicationYear']=publication_extracted.get('date', "").split('-')[0]

    #Date part
    data['schema']['data']['dates'] = []
    date_data = {
            "date": publication_extracted.get('date', ""),
            "datetype": publication_extracted.get('Datetype', "")
        }
    data['schema']['data']['dates'].append(date_data)

    data['schema']['data']['dates'] = []

        # First set of data
    date_data_1 = {
            "date": publication_extracted.get('date', ""), # Get the publication date
            "dateType": "Issued",
        }
    data['schema']['data']['dates'].append(date_data_1)

        # Second set of data
    date_data_2 = {
            "date": datetime.date.today().strftime('%Y-%m-%d'), # Import the current date, and format it
            "dateType": "Updated",
        }
    data['schema']['data']['dates'].append(date_data_2)

    # Laguage part, hard coded
    data['schema']['data']['language']="en"

    #Subject Part
    if types_extracted or keywords_extracted:
        data['schema']['data']['subjects'] = []
        if types_extracted:
            for data_type in types_extracted:
                subject_data = {
                    "subject": f"{data_type.get('type_name', '')}",
                    #"subject valueURI": "",
                    #"schemeURI": "",
                    #"subjectScheme": ""
                }
                data['schema']['data']['subjects'].append(subject_data)
        if keywords_extracted:
            for keyword in keywords_extracted:
                subject_data = {
                "subject": keyword,
                # "subject valueURI": "",
                # "schemeURI": "",
                # "subjectScheme": ""
            }
                data['schema']['data']['subjects'].append(subject_data)

    if samples_extracted:
        seen_common_names = set()  # Due to duplicated 'common_name', thus creating a set to slim down to one unique 'common_name'
        for sample in samples_extracted:
            species_info = sample.get("species", {})
            common_name = species_info.get("scientific_name", "")

            if common_name not in seen_common_names:
                seen_common_names.add(common_name) # Filter
                # A dict with hard coded part
                species_data = {
                    "subject": common_name,
                    "subjectScheme": "NCBI taxonomy",
                    "schemeURI": "https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi",
                    "classificationCode": f"{species_info.get('tax_id', '')}"
                }

                data['schema']['data']['subjects'].append(species_data)


    #RelatedIdentifier part
    if manuscript_links_extracted or external_links_extracted or extract_alternative_identifiers or project_links_extracted:
         data['schema']['data']['relatedIdentifiers'] = []
         processed_types_texts = set()


         if manuscript_links_extracted:
              for link in manuscript_links_extracted:
                manuscript_data = {
                "relatedIdentifier": link.get("manuscript_DOI", ""),
                "resourceTypeGeneral": "JournalArticle",
                "relatedIdentifierType": "DOI",
                "relationType": "IsCitedBy"
            }
                data['schema']['data']['relatedIdentifiers'].append(manuscript_data)

         if alternative_identifiers_extracted:
              for link in alternative_identifiers_extracted:
                alternativeidentifier_data = {
                "relatedIdentifier": link.get("link", ""),
                "resourceTypeGeneral": "Dataset",
                "relatedIdentifierType": "URL",
                "relationType": "References"
            }
                data['schema']['data']['relatedIdentifiers'].append(alternativeidentifier_data)

         if project_links_extracted:
              for link in project_links_extracted:
                project_data={
                "relatedIdentifier": link.get("project_link", ""),
                "resourceTypeGeneral": "Collection",
                "relatedIdentifierType": "URL",
                "relationType": "IsPartOf"

            }
                data['schema']['data']['relatedIdentifiers'].append(project_data)
   # 处理外部链接
         if external_links_extracted:
              for link in external_links_extracted:
                link_type = link.get('type', '')
                link_text = link.get('text', '')
                unique_key = (link_type, link_text)

                # Default setting
                if unique_key not in processed_types_texts:
                   relatedIdentifier = "Other"
                   resourceTypeGeneral = "Other"
                   relatedIdentifierType = "Other"
                   relationType = "Other"

                # Some distinguished examples
                if "Github links" in link_type:
                    relatedIdentifier = link_text
                    resourceTypeGeneral = "software"
                    relatedIdentifierType = "Github"
                    relationType = "HasPart"

                elif "Additional information" in link_type:
                    relatedIdentifier = link_text
                    resourceTypeGeneral = "Other"
                    relatedIdentifierType = "URL"
                    relationType = "References"

                elif "Protocols.io" in link_type:
                    relatedIdentifier = link_text
                    resourceTypeGeneral = "Other"
                    relatedIdentifierType = "DOI"
                    relationType = "References"

                elif "BioProject" in link_type:
                    relatedIdentifier = "https://www.ncbi.nlm.nih.gov/bioproject/PRJNA792936"
                    resourceTypeGeneral = "Dataset"
                    relatedIdentifierType = "URL"
                    relationType = "References"


                #if in the link, there is 'doi', then relatedIdentifierType should be 'DOI'
                if 'doi' in link_text.lower():
                    relatedIdentifierType = "DOI"

                # A dict
                external_link_data = {
                    "relatedIdentifier": relatedIdentifier,
                    "resourceTypeGeneral": resourceTypeGeneral,
                    "relatedIdentifierType": relatedIdentifierType,
                    "relationType": relationType,
                }
                data['schema']['data']['relatedIdentifiers'].append(external_link_data)
                processed_types_texts.add(unique_key)

    #Dataset size part
    if dataset_size_extracted.get('size') is not None and file_count is not None:
        data['schema']['data']['sizes'] = []

        size_unit = dataset_size_extracted.get('units', '').strip()
        size_info = f"{dataset_size_extracted['size']} {size_unit}" if size_unit else f"{dataset_size_extracted['size']}"
        data['schema']['data']['sizes'].append(size_info)

        file_info = f"{file_count} files"
        data['schema']['data']['sizes'].append(file_info)

    #RightsList part
    data['schema']['data']["rightsList"] = []
    rightslist_data = {
        "lang": "en",
        "rights": "CC0 1.0 UNIVERSAL",
        "rightsUri": "http://creativecommons.org/publicdomain/zero/1.0/",
        "rightsIdentifier": "CC0 1.0 Universal"
    }
    data['schema']['data']["rightsList"].append(rightslist_data)

    #Description part
    if description_extracted:
        data['schema']['data']['descriptions'] = []
        description_data = {
            "lang": "en",
            "description": description_extracted,
            "descriptionType": 'Abstract',
        }
        data['schema']['data']['descriptions'].append(description_data)

    # FundingReferences part
    if grants_extracted:
        data['schema']['data']['fundingReferences'] = []
        for grant in grants_extracted:
            funding_reference_data = {
                "funderName": grant.get("funder_name", ""),
                "awardNumber": grant.get("award", ""),
                "awardTitle": grant.get("comment", "")
            }
            # if there is a funderIdentifier, keep the funderIdentifier and funderIdentifierType, if not, remove all
            if grant.get("fundref_url", "") != 'unknown' or '':
                funding_reference_data["funderIdentifier"] = grant.get("fundref_url", "")
                funding_reference_data["funderIdentifierType"] = "Crossref Funder ID"

            data['schema']['data']['fundingReferences'].append(funding_reference_data)

    return data ##The final dataset

"""# Below is for all files automatic generation"""

#This is the activation function, which is to activate the above extraction function.
def main(response):
    # They are setted to be global to be used in the other function, since it will be very unlikely to change them in other part of functions, thus it shall be ok.
    global title_extracted, keywords_extracted, ftp_site_extracted, prefix, suffix, file_count, description_extracted
    global authors_extracted, types_extracted, dataset_size_extracted, publication_extracted, links, samples_extracted

    # Check if the HTTP request was successful
    if response.status_code != 200:
        print(f"Failed to fetch data. Status code: {response.status_code}")
        return

    # Retrieve the XML content from the response
    xml_content = response.text

    # Clean and extract the title from the XML content
    cleaned_title = clean_xml_content_title(xml_content)
    title_extracted = extract_title(cleaned_title)
    # if title_extracted:
    #     print('Extracted Title:', title_extracted)
    # else:
    #     print('No title information found.')

    # Clean and extract keywords from the XML content
    cleaned_ds_attributes = clean_xml_content_ds_attributes(xml_content)
    keywords_extracted = extract_keys(cleaned_ds_attributes)
    # if keywords_extracted:
    #     print('Extracted Keywords:', keywords_extracted)
    # else:
    #     print('No keywords found.')

    # Clean and extract the FTP site information from the XML content
    cleaned_ftp_site = clean_xml_content_ftp_site(xml_content)
    ftp_site_extracted = extract_ftp_site(cleaned_ftp_site)
    # if ftp_site_extracted:
    #     print('Extracted FTP Site:', ftp_site_extracted)
    # else:
    #     print('No FTP information found.')

    # Match patterns in the FTP site string to extract specific segments
    prefix, suffix = match_ftp_site_patterns(ftp_site_extracted)
    # if prefix and suffix:
    #     print("prefix:", prefix)
    #     print("suffix:", suffix)
    # else:
    #     print("No match found.")

    # Extract the number of file elements from the XML content
    file_count = extract_files(xml_content)
    # if file_count:
    #     print(f"Number of <file> elements: {file_count}")
    # else:
    #     print('Fail to count the file number.')

    # Clean and extract the description from the XML content
    cleaned_description = clean_xml_content_description(xml_content)
    description_extracted = extract_description(cleaned_description)
    # if description_extracted:
    #     print('Extracted Description:', description_extracted)
    # else:
    #     print("No description information found.")

    # Extract authors from the XML content
    authors_extracted = extract_authors(xml_content)
    # print('Extracted Authors:')
    # if authors_extracted:
    #     for author in authors_extracted:
    #         print(author)
    # else:
    #     print("No author information found.")

    # Extract data types from the XML content
    types_extracted = extract_data_types(xml_content)
    # if types_extracted:
    #     for data_type in types_extracted:
    #         print(f"type_name: {data_type['type_name']}, type_id: {data_type['type_id']}")
    # else:
    #     print('No data type information found')

    # Extract dataset size information from the XML content
    dataset_size_extracted = extract_dataset_size(xml_content)
    # if dataset_size_extracted:
    #     print(f"size: {dataset_size_extracted['size']}, units: {dataset_size_extracted['units']}")
    # else:
    #     print('No dataset size found')

    # Extract publication details from the XML content
    publication_extracted = extract_publication(xml_content)
    # if publication_extracted:
    #     print(f"date: {publication_extracted['date']}, publisher_name: {publication_extracted['publisher_name']}")
    # else:
    #     print("No publication information found.")

    # Process links and extract different categories of links from the XML content
    links = process_links(xml_content)
    # if links:
    #     for category, links in links.items():
    #         print(f"\n{category}:")
    #         for link in links:
    #             print(link)
    # else:
    #     print('No links information found')

    # Process and extract sample information from the XML content
    samples_extracted = process_samples(xml_content)
    # if samples_extracted:
    #     print("Samples:")
    #     for sample in samples_extracted:
    #         print(sample)
    # else:
    #     print('No sample information found')

def save_last_processed_doi(doi_number):
    with open("last_processed_doi.txt", "w") as file:
        file.write(doi_number)

def get_last_processed_doi():
    if os.path.exists("last_processed_doi.txt"):
        with open("last_processed_doi.txt", "r") as file:
            return file.read()
    return None

def process_dataset(doi_number):
    url = f'https://gigadb.org/api/dataset?doi={doi_number}' ##If you want a doi, replace this line with the link of the doi
    try:
        response = requests.get(url)
        response.raise_for_status()

        main(response)  # Assuming this function processes the response

        final_data = populate_complete_data_structure(
    title_extracted, keywords_extracted, ftp_site_extracted, file_count, description_extracted, authors_extracted, types_extracted,
    dataset_size_extracted, publication_extracted, manuscript_links_extracted, external_links_extracted, alternative_identifiers_extracted,
    project_links_extracted,  grants_extracted, samples_extracted, data_template)

        json_data = json.dumps(final_data, ensure_ascii=False, indent=2)
        json.loads(json_data)  # Validate JSON data

        file_name = f'gigadb_{doi_number}_json_file.json'
        with open(file_name, 'w', encoding='utf-8') as f:
            f.write(json_data)

        print(json_data)  # Print JSON data

        # Save the DOI being processed
        save_last_processed_doi(doi_number)

    except requests.RequestException as e:
        print(f"Failed to retrieve data from {url}: {e}")
        return  # Return from the function on failure

def resume_processing():
    doi_list = get_doi_list('https://gigadb.org/api/list')
    last_processed_doi = get_last_processed_doi()
    start_processing = True if last_processed_doi is None else False

    for doi_number in doi_list:
        if not start_processing and doi_number == last_processed_doi:
            start_processing = True
        if start_processing:
            process_dataset(doi_number)

    # Remove the state file after processing completes
    if os.path.exists("last_processed_doi.txt"):
        os.remove("last_processed_doi.txt")

# run main function
if __name__ == "__main__":
    resume_processing()

"""# Below is for single doi"""

#This is the activation function, which is to activate the above extraction function.
def main(response):
    # They are setted to be global to be used in the other function, since it will be very unlikely to change them in other part of functions, thus it shall be ok.
    global title_extracted, keywords_extracted, ftp_site_extracted, prefix, suffix, file_count, description_extracted
    global authors_extracted, types_extracted, dataset_size_extracted, publication_extracted, links, samples_extracted

    # Check if the HTTP request was successful
    if response.status_code != 200:
        print(f"Failed to fetch data. Status code: {response.status_code}")
        return

    # Retrieve the XML content from the response
    xml_content = response.text

    # Clean and extract the title from the XML content
    cleaned_title = clean_xml_content_title(xml_content)
    title_extracted = extract_title(cleaned_title)
    # if title_extracted:
    #     print('Extracted Title:', title_extracted)
    # else:
    #     print('No title information found.')

    # Clean and extract keywords from the XML content
    cleaned_ds_attributes = clean_xml_content_ds_attributes(xml_content)
    keywords_extracted = extract_keys(cleaned_ds_attributes)
    # if keywords_extracted:
    #     print('Extracted Keywords:', keywords_extracted)
    # else:
    #     print('No keywords found.')

    # Clean and extract the FTP site information from the XML content
    cleaned_ftp_site = clean_xml_content_ftp_site(xml_content)
    ftp_site_extracted = extract_ftp_site(cleaned_ftp_site)
    # if ftp_site_extracted:
    #     print('Extracted FTP Site:', ftp_site_extracted)
    # else:
    #     print('No FTP information found.')

    # Match patterns in the FTP site string to extract specific segments
    prefix, suffix = match_ftp_site_patterns(ftp_site_extracted)
    # if prefix and suffix:
    #     print("prefix:", prefix)
    #     print("suffix:", suffix)
    # else:
    #     print("No match found.")

    # Extract the number of file elements from the XML content
    file_count = extract_files(xml_content)
    # if file_count:
    #     print(f"Number of <file> elements: {file_count}")
    # else:
    #     print('Fail to count the file number.')

    # Clean and extract the description from the XML content
    cleaned_description = clean_xml_content_description(xml_content)
    description_extracted = extract_description(cleaned_description)
    # if description_extracted:
    #     print('Extracted Description:', description_extracted)
    # else:
    #     print("No description information found.")

    # Extract authors from the XML content
    authors_extracted = extract_authors(xml_content)
    # print('Extracted Authors:')
    # if authors_extracted:
    #     for author in authors_extracted:
    #         print(author)
    # else:
    #     print("No author information found.")

    # Extract data types from the XML content
    types_extracted = extract_data_types(xml_content)
    # if types_extracted:
    #     for data_type in types_extracted:
    #         print(f"type_name: {data_type['type_name']}, type_id: {data_type['type_id']}")
    # else:
    #     print('No data type information found')

    # Extract dataset size information from the XML content
    dataset_size_extracted = extract_dataset_size(xml_content)
    # if dataset_size_extracted:
    #     print(f"size: {dataset_size_extracted['size']}, units: {dataset_size_extracted['units']}")
    # else:
    #     print('No dataset size found')

    # Extract publication details from the XML content
    publication_extracted = extract_publication(xml_content)
    # if publication_extracted:
    #     print(f"date: {publication_extracted['date']}, publisher_name: {publication_extracted['publisher_name']}")
    # else:
    #     print("No publication information found.")

    # Process links and extract different categories of links from the XML content
    links = process_links(xml_content)
    # if links:
    #     for category, links in links.items():
    #         print(f"\n{category}:")
    #         for link in links:
    #             print(link)
    # else:
    #     print('No links information found')

    # Process and extract sample information from the XML content
    samples_extracted = process_samples(xml_content)
    # if samples_extracted:
    #     print("Samples:")
    #     for sample in samples_extracted:
    #         print(sample)
    # else:
    #     print('No sample information found')



def process_dataset(doi_number):
    try:
        url = f'https://gigadb.org/api/dataset?doi={doi_number}'
        response = requests.get(url)
        response.raise_for_status()


        main(response)

        # Populate final data structure (implement this function according to your needs)
        final_data = populate_complete_data_structure(
            title_extracted, keywords_extracted, ftp_site_extracted, file_count, description_extracted, authors_extracted, types_extracted,
            dataset_size_extracted, publication_extracted, manuscript_links_extracted, external_links_extracted, alternative_identifiers_extracted,
            project_links_extracted, grants_extracted, samples_extracted, data_template
        )

        # Convert the final data to JSON
        json_data = json.dumps(final_data, ensure_ascii=False, indent=2)

        # Check JSON validity
        json.loads(json_data)

        # Save JSON data to a file
        file_name = f'gigadb {doi_number} json file.json'
        with open(file_name, 'w', encoding='utf-8') as f:
            f.write(json_data)

        # Print out the JSON data
        print(json_data)

    except requests.RequestException as e:
        print(f"Failed to retrieve data from {url}: {e}")


# Example usage
if __name__ == "__main__":
  process_dataset("100257") # Replace the 6 digits doi number here

